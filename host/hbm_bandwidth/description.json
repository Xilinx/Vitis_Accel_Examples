{
    "testinfo": {
        "profile": "no"
    }, 
    "ndevice": [
        "xilinx_u200_xdma", 
        "xilinx_u200_qdma", 
        "xilinx_u250_xdma", 
        "xilinx_u250_qdma", 
        "xilinx_vcu1525_dynamic", 
        "xilinx_aws-vu9p-f1-04261818", 
        "xilinx:u200:xdma", 
        "xilinx:u200:qdma", 
        "xilinx:u250:xdma", 
        "xilinx:u250:qdma", 
        "xilinx:vcu1525:dynamic", 
        "xilinx:aws-vu9p-f1-04261818:dynamic"
    ], 
    "name": "HBM Bandwidth", 
    "contributors": [
        {
            "url": "http://www.xilinx.com", 
            "group": "Xilinx"
        }
    ], 
    "launch": [
        {
            "cmd_args": "BUILD/krnl_vaddmul.xclbin", 
            "name": "generic launch for all flows"
        }
    ], 
    "platform_type": "pcie", 
    "host": {
        "compiler": {
            "sources": [
                "REPO_DIR/common/includes/xcl2"
            ], 
            "includepaths": [
                "REPO_DIR/common/includes/xcl2"
            ]
        }, 
        "host_exe": "host"
    }, 
    "runtime": [
        "OpenCL"
    ], 
    "containers": [
        {
            "accelerators": [
                {
                    "location": "src/krnl_vaddmul.cpp", 
                    "name": "krnl_vaddmul", 
                    "num_compute_units": "8"
                }
            ], 
            "ldclflags": "--sp krnl_vaddmul_1.m_axi_gmem0:HBM[0] --sp krnl_vaddmul_1.m_axi_gmem1:HBM[1] --sp krnl_vaddmul_1.m_axi_gmem2:HBM[2] --sp krnl_vaddmul_1.m_axi_gmem3:HBM[3] --sp krnl_vaddmul_2.m_axi_gmem0:HBM[4] --sp krnl_vaddmul_2.m_axi_gmem1:HBM[5] --sp krnl_vaddmul_2.m_axi_gmem2:HBM[6] --sp krnl_vaddmul_2.m_axi_gmem3:HBM[7] --sp krnl_vaddmul_3.m_axi_gmem0:HBM[8] --sp krnl_vaddmul_3.m_axi_gmem1:HBM[9] --sp krnl_vaddmul_3.m_axi_gmem2:HBM[10] --sp krnl_vaddmul_3.m_axi_gmem3:HBM[11] --sp krnl_vaddmul_4.m_axi_gmem0:HBM[12] --sp krnl_vaddmul_4.m_axi_gmem1:HBM[13] --sp krnl_vaddmul_4.m_axi_gmem2:HBM[14] --sp krnl_vaddmul_4.m_axi_gmem3:HBM[15] --sp krnl_vaddmul_5.m_axi_gmem0:HBM[16] --sp krnl_vaddmul_5.m_axi_gmem1:HBM[17] --sp krnl_vaddmul_5.m_axi_gmem2:HBM[18] --sp krnl_vaddmul_5.m_axi_gmem3:HBM[19] --sp krnl_vaddmul_6.m_axi_gmem0:HBM[20] --sp krnl_vaddmul_6.m_axi_gmem1:HBM[21] --sp krnl_vaddmul_6.m_axi_gmem2:HBM[22] --sp krnl_vaddmul_6.m_axi_gmem3:HBM[23] --sp krnl_vaddmul_7.m_axi_gmem0:HBM[24] --sp krnl_vaddmul_7.m_axi_gmem1:HBM[25] --sp krnl_vaddmul_7.m_axi_gmem2:HBM[26] --sp krnl_vaddmul_7.m_axi_gmem3:HBM[27] --sp krnl_vaddmul_8.m_axi_gmem0:HBM[28] --sp krnl_vaddmul_8.m_axi_gmem1:HBM[29] --sp krnl_vaddmul_8.m_axi_gmem2:HBM[30] --sp krnl_vaddmul_8.m_axi_gmem3:HBM[31]", 
            "name": "krnl_vaddmul"
        }
    ], 
    "description": [
        "This is a HBM bandwidth check design. Design contains 8 compute units of a kernel which has access to all HBM banks (0:31). Host application allocate buffer into all HBM banks and run these 8 compute units concurrently and measure the overall bandwidth between Kernel and HBM Memory."
    ]
}